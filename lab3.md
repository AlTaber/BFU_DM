# Теоретический разбор: Методы кодирования

## Введение

Методы кодирования делятся на две большие группы:
1. **Помехоустойчивое кодирование** — защита данных от ошибок (Хемминг)
2. **Сжатие данных** — уменьшение объёма информации (RLE, Хаффман, арифметическое)

---

## 1. Код Хемминга

### Что это и зачем нужно?

Код Хемминга — это метод помехоустойчивого кодирования, который позволяет **обнаружить и исправить одиночную ошибку** в блоке данных. Применяется в оперативной памяти (ECC RAM), при передаче данных по ненадёжным каналам.

### Основная идея

К исходным данным добавляются **контрольные (проверочные) биты** в позициях, являющихся степенями двойки: 1, 2, 4, 8, 16, 32...

Каждый контрольный бит отвечает за проверку определённой группы битов.

### Как определить количество контрольных битов?

Для `m` бит данных нужно `r` контрольных бит, где:
```
2^r ≥ m + r + 1
```

| Биты данных (m) | Контрольные биты (r) | Всего бит |
|-----------------|----------------------|-----------|
| 4               | 3                    | 7         |
| 8               | 4                    | 12        |
| 16              | 5                    | 21        |
| 32              | 6                    | 38        |

### Расположение битов

```
Позиция:  1  2  3  4  5  6  7  8  9  10 11 12 ...
Тип:      P1 P2 D1 P4 D2 D3 D4 P8 D5 D6 D7 D8 ...

P — контрольный бит (parity)
D — бит данных
```

### Как вычисляются контрольные биты?

Каждый контрольный бит P на позиции `2^k` проверяет все биты, у которых в двоичном представлении позиции установлен бит `k`.

**P1 (позиция 1 = 2⁰)** проверяет позиции: 1, 3, 5, 7, 9, 11, 13...
```
1  = 0001, 3  = 0011, 5  = 0101, 7  = 0111...
         ^          ^          ^          ^
         бит 0 установлен
```

**P2 (позиция 2 = 2¹)** проверяет позиции: 2, 3, 6, 7, 10, 11, 14...
```
2  = 0010, 3  = 0011, 6  = 0110, 7  = 0111...
        ^         ^         ^         ^
        бит 1 установлен
```

**P4 (позиция 4 = 2²)** проверяет позиции: 4, 5, 6, 7, 12, 13, 14, 15...

**P8 (позиция 8 = 2³)** проверяет позиции: 8-15, 24-31...

Значение контрольного бита = XOR всех проверяемых битов (чтобы общее число единиц было чётным).

### Обнаружение и исправление ошибки

При проверке вычисляем **синдром** — заново считаем каждый контрольный бит:
- Если все проверки дают 0 — ошибок нет
- Если какие-то дают 1 — их сумма указывает на позицию ошибки

**Пример:**
```
P1 = 1 (ошибка)  → +1
P2 = 0 (норма)   → +0
P4 = 1 (ошибка)  → +4
P8 = 0 (норма)   → +0
                   ___
Синдром = 5 → ошибка в бите 5
```

Исправление: просто инвертируем бит на найденной позиции.

### Возможности кода Хемминга

| Кодовое расстояние | Возможности |
|--------------------|-------------|
| d = 3              | Исправить 1 ошибку ИЛИ обнаружить 2 |
| d = 4 (с доп. битом) | Исправить 1 И обнаружить 2 |

### Частые вопросы преподавателя

**В: Почему контрольные биты на степенях двойки?**
О: Потому что каждая позиция однозначно представляется как сумма степеней двойки (двоичное представление). Это позволяет каждому контрольному биту независимо проверять свою группу.

**В: Что если ошибка в самом контрольном бите?**
О: Синдром укажет на его позицию, и мы его исправим. Контрольные биты обрабатываются так же, как и биты данных.

**В: Что если две ошибки?**
О: Стандартный код Хемминга не сможет их корректно исправить — он исправит неправильный бит. Для обнаружения двойных ошибок добавляют общий бит чётности.

---

## 2. Расстояние Хемминга

### Определение

**Расстояние Хемминга** между двумя кодовыми словами — это количество позиций, в которых они различаются.

```
d(10110, 10011) = 3
     ^^    ^^
     различаются в 3 позициях
```

### Минимальное кодовое расстояние

**Минимальное расстояние кода** `d_min` — это наименьшее расстояние между любыми двумя кодовыми словами в коде.

### Связь с обнаружением/исправлением ошибок

| d_min | Обнаружение ошибок | Исправление ошибок |
|-------|--------------------|--------------------|
| 2     | 1                  | 0                  |
| 3     | 2                  | 1                  |
| 4     | 3                  | 1                  |
| 5     | 4                  | 2                  |

**Формулы:**
- Обнаружение `t` ошибок: `d_min ≥ t + 1`
- Исправление `t` ошибок: `d_min ≥ 2t + 1`

### Почему это работает?

**Обнаружение (d=2):**
Если слова отличаются минимум на 2 бита, то одиночная ошибка не превратит одно допустимое слово в другое — получится недопустимое.

```
Допустимые: 000, 011, 101, 110 (d=2)
Ошибка в 000: 001, 010, 100 — все недопустимые → ошибка обнаружена
```

**Исправление (d=3):**
Если слова отличаются минимум на 3 бита, то при одной ошибке искажённое слово будет ближе к исходному, чем к любому другому допустимому.

```
Допустимые: 0000000, 1111111 (d=7)
Ошибка: 0000000 → 0100000
Расстояние до 0000000: 1
Расстояние до 1111111: 6
→ Исправляем в 0000000
```

### Как построить код с заданным расстоянием?

**d ≥ 2:** Добавить бит чётности
```
000 → 0000 (чётное число единиц)
001 → 0011
010 → 0101
011 → 0110
```

**d ≥ 3:** Использовать код Хемминга или специально подобранные комбинации

### Частые вопросы преподавателя

**В: Как найти расстояние Хемминга?**
О: XOR двух слов и посчитать количество единиц (вес Хемминга результата).

**В: Почему для исправления нужно d ≥ 2t+1?**
О: Шар радиуса t вокруг каждого кодового слова не должен пересекаться с шарами других слов. Между центрами должно быть минимум 2t+1.

---

## 3. RLE (Run-Length Encoding)

### Что это?

RLE — простейший алгоритм сжатия **без потерь**. Заменяет последовательности одинаковых символов на пару (количество, символ).

### Принцип работы

```
Исходно:  AAAAAABBBCC
Сжато:    6A3B2C
```

### Формат из задания

```
Повторы:        N, символ       (N > 0, повторить символ N раз)
Неповторяющиеся: 0, K, символы  (K штук разных символов)
```

**Пример:**
```
aaaaaaadghtttttt
↓
7, 'a', 0, 3, 'd', 'g', 'h', 6, 't'
```

### Когда эффективен?

✅ Хорошо сжимает:
- Изображения с большими однотонными областями
- Факсимильные сообщения
- Простую графику, иконки

❌ Плохо сжимает (может даже увеличить!):
- Фотографии
- Случайные данные
- Текст (мало повторов)

### Оценка эффективности

**Лучший случай:** Строка из одного символа
```
"AAAA...A" (255 символов) → 2 байта
Сжатие в 127 раз
```

**Худший случай:** Все символы разные
```
"ABCDEF" (6 байт) → 0, 6, A, B, C, D, E, F (8 байт)
Увеличение на 33%
```

### Частые вопросы преподавателя

**В: Это сжатие с потерями или без?**
О: Без потерь — исходные данные восстанавливаются полностью.

**В: Где применяется RLE?**
О: BMP, TIFF, PCX форматы, факсы (стандарт ITU-T), как этап в других алгоритмах (JPEG, PNG).

**В: Почему нужен маркер 0 для неповторяющихся?**
О: Чтобы отличить от повторяющихся. Если первый байт > 0, это счётчик повторов. Если = 0, дальше идёт длина неповторяющейся последовательности.

---

## 4. Алгоритм Хаффмана

### Что это?

Алгоритм Хаффмана — метод построения **оптимального префиксного кода** с переменной длиной. Часто встречающиеся символы кодируются короче, редкие — длиннее.

### Свойство префиксности

Никакой код не является началом другого кода. Это позволяет однозначно декодировать без разделителей.

```
A = 0, B = 10, C = 11 — префиксный (OK)
A = 0, B = 01, C = 010 — НЕ префиксный (B — префикс C)
```

### Алгоритм построения

1. Создать узел для каждого символа с его частотой
2. Пока узлов больше одного:
   - Взять два узла с минимальной частотой
   - Объединить в новый узел с суммарной частотой
   - Добавить новый узел в очередь
3. Оставшийся узел — корень дерева
4. Левая ветвь = 0, правая = 1
5. Код символа = путь от корня до листа

### Пример из задания

```
Частоты: A=1, B=2, C=9, D=9, E=19, F=27, G=33

Шаг 1: A(1) + B(2) = AB(3)
Шаг 2: AB(3) + C(9) = ABC(12)
Шаг 3: D(9) + ABC(12) = ABCD(21)
Шаг 4: E(19) + ABCD(21) = ABCDE(40)
Шаг 5: F(27) + G(33) = FG(60)
Шаг 6: ABCDE(40) + FG(60) = корень(100)
```

### Результат (примерный)

```
G: 1      (33/100 — самый частый, самый короткий код)
F: 01
E: 000
D: 0010
C: 00111
B: 001101
A: 001100
```

### Почему это оптимально?

Хаффман минимизирует **среднюю длину кода**:
```
L = Σ (частота_i × длина_кода_i)
```

Доказательство оптимальности основано на том, что:
1. Редкие символы должны быть глубже в дереве
2. Два самых редких символа — братья на максимальной глубине

### Сравнение с равномерным кодом

```
7 символов → равномерный код: 3 бита на символ
100 символов → 300 бит равномерно

Хаффман: 1×4 + 2×4 + 9×3 + 9×3 + 19×3 + 27×2 + 33×1 = 
         4 + 8 + 27 + 27 + 57 + 54 + 33 = 210 бит

Сжатие: 300/210 = 1.43 раза
```

### Частые вопросы преподавателя

**В: Почему код Хаффмана называется префиксным?**
О: Ни один код не является префиксом другого, что позволяет однозначно декодировать поток бит.

**В: Единственно ли дерево Хаффмана?**
О: Нет. При равных частотах можно выбирать порядок объединения по-разному. Но средняя длина будет одинаковой.

**В: Где хранится таблица кодов?**
О: Должна передаваться вместе с данными или храниться в заголовке файла. Это накладные расходы.

**В: Что лучше — Хаффман или арифметическое?**
О: Арифметическое ближе к теоретическому пределу (энтропии), но сложнее. Хаффман проще и работает посимвольно.

---

## 5. Арифметическое кодирование

### Что это?

Арифметическое кодирование — метод сжатия, представляющий **всё сообщение одним числом** из интервала [0, 1). В отличие от Хаффмана, не присваивает код каждому символу отдельно.

### Основная идея

1. Начинаем с интервала [0, 1)
2. Делим его на части пропорционально вероятностям символов
3. Для каждого символа сообщения сужаем интервал до части, соответствующей этому символу
4. Итоговый интервал представляем минимальным количеством бит

### Пример из задания

```
Алфавит: a, b, c, d, e, f
Вероятности: 0.10, 0.10, 0.05, 0.55, 0.10, 0.10

Интервалы:
a: [0.00, 0.10)
b: [0.10, 0.20)
c: [0.20, 0.25)
d: [0.25, 0.80)
e: [0.80, 0.90)
f: [0.90, 1.00)
```

### Процесс кодирования "aecdfb"

```
Начало: [0, 1)

'a': выбираем [0.00, 0.10) → [0, 0.1)
'e': в [0, 0.1) часть e это [0.08, 0.09) → [0.08, 0.09)
'c': в [0.08, 0.09) часть c → [0.082, 0.0825)
'd': в этом интервале часть d → [0.08225, 0.0849)
'f': → [0.082466, 0.082490)
'b': → итоговый интервал
```

### Перевод в двоичный код

Находим кратчайшую двоичную дробь, попадающую в итоговый интервал:

```
0.082... в двоичном виде ≈ 0.0001010...
```

Добавляем биты, пока дробь не попадёт в интервал.

### Почему это эффективно?

Арифметическое кодирование приближается к **теоретическому пределу сжатия** — энтропии:

```
H = -Σ p_i × log₂(p_i)
```

Для нашего алфавита:
```
H = -(0.1×log₂0.1×4 + 0.05×log₂0.05 + 0.55×log₂0.55)
  ≈ 2.05 бит на символ
```

Хаффман даёт целое число бит на символ, арифметическое — дробное (в среднем).

### Сравнение с Хаффманом

| Критерий | Хаффман | Арифметическое |
|----------|---------|----------------|
| Эффективность | ~оптимально | ближе к энтропии |
| Сложность | простой | сложнее |
| Единица | символ | всё сообщение |
| Патенты | нет | были до 2000-х |

### Проблемы реализации

1. **Точность:** интервал быстро сужается → нужна высокая точность
2. **Переполнение:** решается масштабированием на лету
3. **Конец сообщения:** нужен маркер или передача длины

### Частые вопросы преподавателя

**В: Почему арифметическое кодирование лучше Хаффмана?**
О: Потому что Хаффман выделяет целое число бит на символ (минимум 1), а арифметическое может "тратить" дробное количество бит, приближаясь к энтропии.

**В: Как декодировать?**
О: Зная число и вероятности, определяем, в какой интервал оно попадает → первый символ. Затем масштабируем число к этому интервалу и повторяем.

**В: Когда арифметическое кодирование особенно выигрывает?**
О: Когда вероятности сильно неравномерны и далеки от степеней 1/2. Например, если P(a)=0.99, Хаффман даст 1 бит, а арифметическое ≈ 0.08 бит.

---

## Сводная таблица методов

| Метод | Тип | Назначение | Сложность |
|-------|-----|------------|-----------|
| Хемминг | Помехоустойчивый | Исправление 1 ошибки | O(n log n) |
| RLE | Сжатие | Последовательности повторов | O(n) |
| Хаффман | Сжатие | Оптимальный префиксный код | O(n log n) |
| Арифметическое | Сжатие | Приближение к энтропии | O(n) |

---

## Формулы для запоминания

### Код Хемминга
```
Контрольных бит: 2^r ≥ m + r + 1
Синдром = P1×1 + P2×2 + P4×4 + P8×8 + ...
```

### Расстояние Хемминга
```
Обнаружение t ошибок: d ≥ t + 1
Исправление t ошибок: d ≥ 2t + 1
```

### Коэффициенты сжатия
```
Коэффициент сжатия = исходный_размер / сжатый_размер
Степень сжатия = (1 - сжатый/исходный) × 100%
```

### Энтропия (теоретический предел)
```
H = -Σ p_i × log₂(p_i) бит/символ
```

### Средняя длина кода Хаффмана
```
L = Σ p_i × l_i, где l_i — длина кода символа i
```